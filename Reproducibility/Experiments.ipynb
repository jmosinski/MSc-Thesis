{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unlikely-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import StackingRegressor, VotingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_context(font_scale=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "original-metropolitan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peptide</th>\n",
       "      <th>peptide_aromaticity</th>\n",
       "      <th>peptide_gravy</th>\n",
       "      <th>peptide_helix</th>\n",
       "      <th>peptide_instability_index</th>\n",
       "      <th>peptide_isoelectric_point</th>\n",
       "      <th>peptide_len</th>\n",
       "      <th>peptide_log_len</th>\n",
       "      <th>peptide_log_weight</th>\n",
       "      <th>peptide_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>protein_len</th>\n",
       "      <th>protein_log_len</th>\n",
       "      <th>protein_log_weight</th>\n",
       "      <th>protein_molar_extinction_coefficient</th>\n",
       "      <th>protein_sheet</th>\n",
       "      <th>protein_turn</th>\n",
       "      <th>protein_weight</th>\n",
       "      <th>reproducibility</th>\n",
       "      <th>reproducibility_corrected</th>\n",
       "      <th>peptide_detectability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELTQQLNALFQDK</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>-0.742857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>39.107143</td>\n",
       "      <td>4.368781</td>\n",
       "      <td>14</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>7.399267</td>\n",
       "      <td>21.571060</td>\n",
       "      <td>...</td>\n",
       "      <td>480</td>\n",
       "      <td>6.173786</td>\n",
       "      <td>10.925809</td>\n",
       "      <td>14900.0</td>\n",
       "      <td>0.372917</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>55592.8205</td>\n",
       "      <td>0.134116</td>\n",
       "      <td>-0.416929</td>\n",
       "      <td>0.863536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EETGQVLER</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.288889</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>4.252773</td>\n",
       "      <td>9</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>6.966134</td>\n",
       "      <td>23.123484</td>\n",
       "      <td>...</td>\n",
       "      <td>613</td>\n",
       "      <td>6.418365</td>\n",
       "      <td>11.114425</td>\n",
       "      <td>114625.0</td>\n",
       "      <td>0.225122</td>\n",
       "      <td>0.295269</td>\n",
       "      <td>67132.6103</td>\n",
       "      <td>0.118270</td>\n",
       "      <td>-0.276561</td>\n",
       "      <td>0.539523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QLYGDTGVLGR</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.263636</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-20.009091</td>\n",
       "      <td>5.835682</td>\n",
       "      <td>11</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>7.071824</td>\n",
       "      <td>22.152872</td>\n",
       "      <td>...</td>\n",
       "      <td>184</td>\n",
       "      <td>5.214936</td>\n",
       "      <td>9.929104</td>\n",
       "      <td>22982.5</td>\n",
       "      <td>0.288043</td>\n",
       "      <td>0.163043</td>\n",
       "      <td>20518.9427</td>\n",
       "      <td>0.730757</td>\n",
       "      <td>-0.019208</td>\n",
       "      <td>0.858968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EVQVFEITENSAK</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>-0.415385</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>33.061538</td>\n",
       "      <td>4.252773</td>\n",
       "      <td>13</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>7.308954</td>\n",
       "      <td>18.674903</td>\n",
       "      <td>...</td>\n",
       "      <td>188</td>\n",
       "      <td>5.236442</td>\n",
       "      <td>9.948520</td>\n",
       "      <td>4470.0</td>\n",
       "      <td>0.218085</td>\n",
       "      <td>0.202128</td>\n",
       "      <td>20921.2417</td>\n",
       "      <td>0.245870</td>\n",
       "      <td>0.106907</td>\n",
       "      <td>0.876252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EDYICYAR</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.662500</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>62.425000</td>\n",
       "      <td>4.370430</td>\n",
       "      <td>8</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>6.939377</td>\n",
       "      <td>23.851474</td>\n",
       "      <td>...</td>\n",
       "      <td>127</td>\n",
       "      <td>4.844187</td>\n",
       "      <td>9.523820</td>\n",
       "      <td>18450.0</td>\n",
       "      <td>0.188976</td>\n",
       "      <td>0.377953</td>\n",
       "      <td>13681.7806</td>\n",
       "      <td>0.768097</td>\n",
       "      <td>0.377618</td>\n",
       "      <td>0.649913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          peptide  peptide_aromaticity  peptide_gravy  peptide_helix  \\\n",
       "0  SELTQQLNALFQDK             0.071429      -0.742857       0.285714   \n",
       "1       EETGQVLER             0.000000      -1.288889       0.222222   \n",
       "2     QLYGDTGVLGR             0.090909      -0.263636       0.363636   \n",
       "3   EVQVFEITENSAK             0.076923      -0.415385       0.307692   \n",
       "4        EDYICYAR             0.250000      -0.662500       0.375000   \n",
       "\n",
       "   peptide_instability_index  peptide_isoelectric_point  peptide_len  \\\n",
       "0                  39.107143                   4.368781           14   \n",
       "1                  27.300000                   4.252773            9   \n",
       "2                 -20.009091                   5.835682           11   \n",
       "3                  33.061538                   4.252773           13   \n",
       "4                  62.425000                   4.370430            8   \n",
       "\n",
       "   peptide_log_len  peptide_log_weight  peptide_mean  ...  protein_len  \\\n",
       "0         2.639057            7.399267     21.571060  ...          480   \n",
       "1         2.197225            6.966134     23.123484  ...          613   \n",
       "2         2.397895            7.071824     22.152872  ...          184   \n",
       "3         2.564949            7.308954     18.674903  ...          188   \n",
       "4         2.079442            6.939377     23.851474  ...          127   \n",
       "\n",
       "   protein_log_len  protein_log_weight  protein_molar_extinction_coefficient  \\\n",
       "0         6.173786           10.925809                               14900.0   \n",
       "1         6.418365           11.114425                              114625.0   \n",
       "2         5.214936            9.929104                               22982.5   \n",
       "3         5.236442            9.948520                                4470.0   \n",
       "4         4.844187            9.523820                               18450.0   \n",
       "\n",
       "   protein_sheet  protein_turn  protein_weight  reproducibility  \\\n",
       "0       0.372917      0.162500      55592.8205         0.134116   \n",
       "1       0.225122      0.295269      67132.6103         0.118270   \n",
       "2       0.288043      0.163043      20518.9427         0.730757   \n",
       "3       0.218085      0.202128      20921.2417         0.245870   \n",
       "4       0.188976      0.377953      13681.7806         0.768097   \n",
       "\n",
       "  reproducibility_corrected  peptide_detectability  \n",
       "0                 -0.416929               0.863536  \n",
       "1                 -0.276561               0.539523  \n",
       "2                 -0.019208               0.858968  \n",
       "3                  0.106907               0.876252  \n",
       "4                  0.377618               0.649913  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/reproducibility.csv').sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alpine-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsContainer:\n",
    "    def __init__(self, path=None):\n",
    "        self.container = defaultdict(\n",
    "            lambda: defaultdict(\n",
    "                lambda: dict(\n",
    "                    optim_history=[],\n",
    "                    params=defaultdict(list),\n",
    "                    results=defaultdict(list),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        if path is not None:\n",
    "            self.load(path)\n",
    "\n",
    "    def update_from_list(self, exp_name, optim_history, params, results):\n",
    "        for model_name in optim_history:\n",
    "            self.update(exp_name, model_name,\n",
    "                        optim_history[model_name],\n",
    "                        params[model_name],\n",
    "                        results[model_name])\n",
    "\n",
    "    def update(self, exp_name, model_name, optim_history, params, results):\n",
    "        self.container[exp_name][model_name]['optim_history'].append(optim_history)\n",
    "        for k, v in params.items():\n",
    "            self.container[exp_name][model_name]['params'][k].append(v)\n",
    "        for k, v in results.items():\n",
    "            self.container[exp_name][model_name]['results'][k].append(v)\n",
    "        \n",
    "    def save(self, path):\n",
    "        pickle.dump(self.to_dict(), open(path, 'wb'))\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.container.update(pickle.load(open(path, 'rb')))\n",
    "        \n",
    "    def update_file(self, path):\n",
    "        container = ResultsContainer(path)\n",
    "        container_new = self.container\n",
    "        for exp_name, models in container_new.items():\n",
    "            for model_name, model_container in models.items():\n",
    "                container[exp_name][model_name] = model_container\n",
    "        container.save(path)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.rec_to_dict(self.container)\n",
    "    \n",
    "    def rec_to_dict(self, d):\n",
    "        if type(d)!=defaultdict and type(d)!=dict:\n",
    "            return d\n",
    "        for k, v in d.items():\n",
    "            if type(v)==defaultdict or type(v)==dict:\n",
    "                d[k] = self.rec_to_dict(v)\n",
    "        return dict(d)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.container[key]\n",
    "    \n",
    "\n",
    "def get_data(df, features=[]):\n",
    "    xs = {}\n",
    "    encoder = CountVectorizer(analyzer='char', lowercase=False)\n",
    "    xs['Counts'] = encoder.fit_transform(df.peptide.values).toarray()\n",
    "    xs['Binary'] = np.where(xs['Counts']>0, 1, 0)\n",
    "    xs['Relative'] = xs['Counts'] / xs['Counts'].sum(1, keepdims=True)\n",
    "    \n",
    "    embeds_path = '../Data/Embeds/'\n",
    "    for name, file in (('Doc2Vec', 'from_model_3_1.pkl'), ('Bert', 'bert_embeds.pkl')):\n",
    "        embeds = df.peptide.map(pickle.load(open(embeds_path+file, 'rb'))).values\n",
    "        xs[name] = np.vstack(embeds)\n",
    "    \n",
    "    if features:\n",
    "        x_meta = df[features].values.reshape(-1, len(features))\n",
    "        for key, x in xs.items():\n",
    "            xs[key] = np.hstack([x, x_meta])\n",
    "        xs['Meta'] = x_meta\n",
    "\n",
    "    y = df.reproducibility.values\n",
    "    groups = LabelEncoder().fit_transform(df['protein_id'].values)\n",
    "    features =  list(encoder.get_feature_names()) + features\n",
    "    return xs, y, groups, features \n",
    "\n",
    "def hyperopt(xs, y, Model, space, evals=100, cv_params={}, params={}, int_params={}, Scaler=None):\n",
    "    def objective(space):\n",
    "        for key, value in space.items():\n",
    "            if key != 'features':\n",
    "                if key in int_params:\n",
    "                    params[key] = int(value)\n",
    "                else:\n",
    "                    params[key] = value\n",
    "        \n",
    "        if Scaler is None:\n",
    "            model = Model(**params)\n",
    "        else:\n",
    "            model = make_pipeline(Scaler(), Model(**params))\n",
    "            \n",
    "        if 'features' in space:\n",
    "            x = xs[space['features']]\n",
    "        else:\n",
    "            x = xs\n",
    "\n",
    "        scores = -cross_val_score(model, x, y, **cv_params)\n",
    "        return scores.mean() \n",
    "    \n",
    "    params = params.copy()\n",
    "    int_params = int_params.copy()\n",
    "    default_params = params.copy()\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=objective,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=evals,\n",
    "                trials=trials)\n",
    "\n",
    "    history = defaultdict(list)\n",
    "    for t in trials:\n",
    "        history['loss'].append(t['result']['loss'])\n",
    "        for k, v in default_params.items():\n",
    "            history[k].append(v)\n",
    "        for k, v in t['misc']['vals'].items():\n",
    "            if k in int_params:\n",
    "                history[k].append(int(v[0]))\n",
    "            else:\n",
    "                history[k].append(v[0])\n",
    "\n",
    "    for k, v in best.items():\n",
    "        if k in int_params:\n",
    "            params[k] = int(v)\n",
    "        else:\n",
    "            params[k] = v\n",
    "    \n",
    "    return params, dict(history)\n",
    "\n",
    "def optimize_models(x, y, model_optim_params, cv_params, evals=1):\n",
    "    optim_params, optim_history = {}, {}\n",
    "    for model_name, model_params in model_optim_params.items():\n",
    "        params, history = hyperopt(x, y, **model_params,\n",
    "                                   cv_params=cv_params, evals=evals)\n",
    "        optim_params[model_name] = params\n",
    "        optim_history[model_name] = history\n",
    "    return optim_params, optim_history\n",
    "\n",
    "def get_model(Model, optim_params, optim_results):\n",
    "    model_name = Model.__name__\n",
    "    if model_name in optim_results:\n",
    "        if model_name == 'XGBRegressor':\n",
    "            optim_results[model_name]['random_state'] = np.random.randint(1000)\n",
    "        if model_name in optim_params:\n",
    "            model = optim_params[model_name]['Model'](**optim_results[model_name])\n",
    "        else:\n",
    "            model = Model(**optim_results[model_name])\n",
    "        if 'Scaler' in optim_params[model_name]:\n",
    "            scaler = optim_params[model_name]['Scaler']()\n",
    "            return model_name, make_pipeline(scaler, model)\n",
    "        else:\n",
    "            return model_name, model\n",
    "    else:\n",
    "        return model_name, Model()\n",
    "\n",
    "def compare_models(x_train, y_train, x_test, y_test,\n",
    "                   model_classes, optim_params, optim_results):\n",
    "    metrics = dict(\n",
    "        MSE=mean_squared_error,\n",
    "        MAE=mean_absolute_error,\n",
    "        R2=r2_score,\n",
    "        EV=explained_variance_score\n",
    "    )\n",
    "    results = defaultdict(dict)\n",
    "    for Model in model_classes:\n",
    "        model_name, model = get_model(Model, optim_params, optim_results)\n",
    "        preds_train = model.fit(x_train, y_train).predict(x_train)\n",
    "        preds_test = model.predict(x_test)\n",
    "        for metric, func in metrics.items():\n",
    "            results[model_name]['Train '+metric] = func(y_train, preds_train)\n",
    "            results[model_name]['Test '+metric] = func(y_test, preds_test)\n",
    "    return dict(results)\n",
    "\n",
    "def print_table(exp_names, comp_results, ci=False, precision=5):\n",
    "    model_names = comp_results[exp_names[0]]['Model']\n",
    "    lines = []\n",
    "    lines.append('\\\\begin{table}[h]')\n",
    "    lines.append('\\\\centering')\n",
    "    lines.append('\\\\begin{tabular}{l' + '|l'*len(exp_names) + '}')\n",
    "    lines.append('\\\\textbf{Model}' + ''.join([' & \\\\textbf{'+e+'}' for e in exp_names]) + ' \\\\\\\\ \\\\hline')\n",
    "    lines += [model for model in model_names]\n",
    "    for exp_name in exp_names:\n",
    "        for i in range(len(model_names)):\n",
    "            mse = comp_results[exp_name]['Test MSE'][i]\n",
    "            if ci:\n",
    "                std = comp_results[exp_name]['Test Std'][i]\n",
    "                text = str(np.round(mse, precision)) + ' \\\\pm ' + str(np.round(std, precision))\n",
    "            else:\n",
    "                text = str(np.round(mse, precision))\n",
    "            text = ' & ' + text\n",
    "            lines[i+4] += text\n",
    "    for i in range(len(model_names)-1):\n",
    "        lines[i+4] += ' \\\\\\\\ \\\\hline'\n",
    "    lines.append('\\\\end{tabular}')\n",
    "    lines.append('\\\\caption{}')\n",
    "    lines.append('\\\\label{}')\n",
    "    lines.append('\\\\end{table}')\n",
    "    lines = '\\n'.join(lines)\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "quiet-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimization parameter space\n",
    "optim_params = {}\n",
    "optim_params['KNeighborsRegressor'] = dict(\n",
    "    Model = KNeighborsRegressor,\n",
    "    space = {\n",
    "        'n_neighbors': hp.qloguniform('n_neighbors', np.log(1), np.log(300), 1),\n",
    "    },\n",
    "    int_params={'n_neighbors'},\n",
    "    Scaler=StandardScaler,\n",
    ")\n",
    "optim_params['KernelRidge'] =  dict(\n",
    "    Model = KernelRidge,\n",
    "    space = {\n",
    "    'alpha': hp.loguniform('alpha', np.log(0.1), np.log(100)),\n",
    "    'degree': hp.quniform('degree', 1, 4, 1),\n",
    "    },\n",
    "    params = {'kernel': 'poly'},\n",
    "    int_params={'degree'},\n",
    "    Scaler=StandardScaler,\n",
    ")\n",
    "optim_params['Ridge'] =  dict(\n",
    "    Model = lambda degree, alpha: make_pipeline(\n",
    "        PolynomialFeatures(degree, include_bias=False),\n",
    "        Ridge(alpha=alpha)\n",
    "    ),\n",
    "    space = {\n",
    "        'alpha': hp.loguniform('alpha', np.log(1e2), np.log(1e5)),\n",
    "        'degree': hp.quniform('degree', 1, 3, 1),\n",
    "    },\n",
    "    int_params={'degree'},\n",
    "    Scaler=StandardScaler,\n",
    ")\n",
    "optim_params['ElasticNet'] =  dict(\n",
    "    Model = lambda degree, alpha, l1_ratio: make_pipeline(\n",
    "        PolynomialFeatures(degree, include_bias=False),\n",
    "        ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "    ),\n",
    "    space = {\n",
    "        'alpha': hp.loguniform('alpha', np.log(1e-4), np.log(10)),\n",
    "        'l1_ratio': hp.loguniform('l1_ratio', np.log(1e-4), np.log(1)),\n",
    "        'degree': hp.quniform('degree', 1, 3, 1),\n",
    "    },\n",
    "    int_params={'degree'},\n",
    "    Scaler=StandardScaler,\n",
    ")\n",
    "optim_params['RandomForestRegressor'] = dict(\n",
    "    Model=RandomForestRegressor,\n",
    "    space={\n",
    "        'ccp_alpha': hp.loguniform('ccp_alpha', np.log(1e-6), np.log(1e-3)),\n",
    "        'max_features': hp.uniform('max_features', 0.1, 1),\n",
    "    },\n",
    "    params = {'n_estimators': 300},\n",
    "    int_params = {'max_depth', 'min_samples_leaf', 'n_estimators'},\n",
    ")\n",
    "optim_params['XGBRegressor'] = dict(\n",
    "    Model=XGBRegressor,\n",
    "    space={\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.3)),\n",
    "        'colsample_bytree': hp.uniform ('colsample_bytree', 0.1, 1),\n",
    "        'colsample_bylevel': hp.uniform ('colsample_bylevel', 0.1, 1),\n",
    "        'subsample': hp.uniform ('subsample', 0.3, 1),\n",
    "        'max_depth': hp.quniform('max_depth', 2, 8, 1),\n",
    "#         'n_estimators': hp.qloguniform('n_estimators', np.log(100), np.log(1000), 10),\n",
    "        'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-4), np.log(10)),\n",
    "        'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-4), np.log(10)),\n",
    "    },\n",
    "    params = {'n_estimators': 300},\n",
    "    int_params={'min_child_weight', 'max_depth', 'n_estimators'},\n",
    ")\n",
    "optim_params['MLPRegressor'] = dict(\n",
    "    Model=MLPRegressor,\n",
    "    space = {\n",
    "        'hidden_layer_sizes': hp.qloguniform('hidden_layer_sizes', np.log(8), np.log(256), 2),\n",
    "        'learning_rate_init': hp.loguniform('learning_rate_init', np.log(1e-3), np.log(1)),\n",
    "        'alpha': hp.loguniform('alpha', np.log(1e-2), np.log(10)),\n",
    "        'batch_size': hp.qloguniform('batch_size', np.log(8), np.log(256), 4),\n",
    "    },\n",
    "    params = {'solver':'sgd', 'learning_rate': 'adaptive'},\n",
    "    int_params = {'hidden_layer_sizes', 'batch_size'},\n",
    "    Scaler=StandardScaler,\n",
    ")\n",
    "\n",
    "# Define experiments\n",
    "experiment_params = {}\n",
    "# # SEQ\n",
    "experiment_params['Counts'] = dict(\n",
    "    encoding_type='Counts',\n",
    "    features=[],\n",
    ")\n",
    "experiment_params['Relative'] = dict(\n",
    "    encoding_type='Relative',\n",
    "    features=[],\n",
    ")\n",
    "experiment_params['Binary'] = dict(\n",
    "    encoding_type='Binary',\n",
    "    features=[],\n",
    ")\n",
    "# SEQ + len\n",
    "experiment_params['Counts+len'] = dict(\n",
    "    encoding_type='Counts',\n",
    "    features=['peptide_len'],\n",
    ")\n",
    "experiment_params['Relative+len'] = dict(\n",
    "    encoding_type='Relative',\n",
    "    features=['peptide_len'],\n",
    ")\n",
    "experiment_params['Binary+len'] = dict(\n",
    "    encoding_type='Binary',\n",
    "    features=['peptide_len'],\n",
    ")\n",
    "# Embeds\n",
    "experiment_params['Doc2Vec'] = dict(\n",
    "    encoding_type='Doc2Vec',\n",
    "    features=[],\n",
    ")\n",
    "experiment_params['Bert'] = dict(\n",
    "    encoding_type='Bert',\n",
    "    features=[],\n",
    ")\n",
    "# Peptide\n",
    "features = [\n",
    "    'len', 'gravy', 'helix', 'turn', 'sheet', 'aromaticity',\n",
    "    'instability_index', 'isoelectric_point', 'molar_extinction_coefficient'\n",
    "]\n",
    "features = ['peptide_' + f for f in features] + ['peptide_rt', 'peptide_detectability',\n",
    "                                                 'peptide_mean', 'peptide_mean_diff']\n",
    "experiment_params['Peptide'] = dict(\n",
    "    encoding_type='Meta',\n",
    "    features=features,\n",
    ")\n",
    "# Peptide+Protein \n",
    "features = [\n",
    "    'len', 'gravy', 'helix', 'turn', 'sheet', 'aromaticity',\n",
    "    'instability_index', 'isoelectric_point', 'molar_extinction_coefficient'\n",
    "]\n",
    "peptide_features = ['peptide_' + f for f in features] + ['peptide_rt', 'peptide_detectability',\n",
    "                                                         'peptide_mean', 'peptide_mean_diff']\n",
    "protein_features = ['protein_' + f for f in features]\n",
    "features = peptide_features + protein_features\n",
    "experiment_params['Peptide+Protein'] = dict(\n",
    "    encoding_type='Meta',\n",
    "    features=features,\n",
    ")\n",
    "# Counts+Peptide\n",
    "features = [\n",
    "    'len', 'gravy', 'helix', 'turn', 'sheet', 'aromaticity',\n",
    "    'instability_index', 'isoelectric_point', 'molar_extinction_coefficient'\n",
    "]\n",
    "features = ['peptide_' + f for f in features] + ['peptide_rt', 'peptide_detectability',\n",
    "                                                 'peptide_mean', 'peptide_mean_diff']\n",
    "experiment_params['Counts+Peptide'] = dict(\n",
    "    encoding_type='Counts',\n",
    "    features=features,\n",
    ")\n",
    "# Counts+Peptide+Protein\n",
    "features = [\n",
    "    'len', 'gravy', 'helix', 'turn', 'sheet', 'aromaticity',\n",
    "    'instability_index', 'isoelectric_point', 'molar_extinction_coefficient'\n",
    "]\n",
    "peptide_features = ['peptide_' + f for f in features] + ['peptide_rt', 'peptide_detectability',\n",
    "                                                         'peptide_mean', 'peptide_mean_diff']\n",
    "protein_features = ['protein_' + f for f in features]\n",
    "features = peptide_features + protein_features\n",
    "experiment_params['Counts+Peptide+Protein'] = dict(\n",
    "    encoding_type='Counts',\n",
    "    features=features,\n",
    ")\n",
    "# Important\n",
    "features = [\n",
    "    'peptide_len', 'peptide_mean', 'peptide_mean_diff',\n",
    "    'peptide_detectability', 'peptide_rt', \n",
    "]\n",
    "experiment_params['Important'] = dict(\n",
    "    encoding_type='Counts',\n",
    "    features=features,\n",
    ")\n",
    "\n",
    "# Models to test and exp to run\n",
    "models_to_test = [\n",
    "#     LinearRegression,\n",
    "#     DummyRegressor,\n",
    "#     KNeighborsRegressor,\n",
    "#     Ridge,\n",
    "#     ElasticNet,\n",
    "    KernelRidge,\n",
    "    RandomForestRegressor,\n",
    "    MLPRegressor,\n",
    "#     XGBRegressor,\n",
    "]\n",
    "exp_to_run = [\n",
    "#     'Counts',\n",
    "#     'Relative',\n",
    "#     'Binary',\n",
    "#     'Counts+len',\n",
    "#     'Relative+len',\n",
    "#     'Binary+len',\n",
    "#     'Doc2Vec',\n",
    "#     'Bert',\n",
    "#     'Peptide',\n",
    "#     'Peptide+Protein',\n",
    "#     'Counts+Peptide',\n",
    "#     'Counts+Peptide+Protein'\n",
    "    'Important'\n",
    "]\n",
    "\n",
    "# Get only relevant exp and models\n",
    "names = set([m.__name__ for m in models_to_test])\n",
    "to_del = []\n",
    "for k in optim_params:\n",
    "    if k not in names:\n",
    "        to_del.append(k)\n",
    "for k in to_del:\n",
    "    del optim_params[k]\n",
    "\n",
    "experiment_params = {k:experiment_params[k] for k in exp_to_run}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-circular",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extensive-arabic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important\n",
      "100%|██████████████████████████████████████████████| 30/30 [00:30<00:00,  1.00s/trial, best loss: 0.052365128651546175]\n",
      "100%|███████████████████████████████████████████████| 30/30 [05:06<00:00, 10.20s/trial, best loss: 0.04937882715190304]\n",
      "100%|███████████████████████████████████████████████| 30/30 [03:38<00:00,  7.27s/trial, best loss: 0.05051446586860155]\n",
      "100%|███████████████████████████████████████████████| 30/30 [00:27<00:00,  1.10trial/s, best loss: 0.05384456749319359]\n",
      "100%|██████████████████████████████████████████████| 30/30 [05:16<00:00, 10.56s/trial, best loss: 0.049600237791950816]\n",
      "100%|███████████████████████████████████████████████| 30/30 [04:08<00:00,  8.28s/trial, best loss: 0.05107608775935883]\n",
      "100%|█████████████████████████████████████████████████| 30/30 [00:23<00:00,  1.28trial/s, best loss: 0.052496668650418]\n",
      "100%|██████████████████████████████████████████████| 30/30 [05:12<00:00, 10.41s/trial, best loss: 0.048898106056645595]\n",
      "100%|███████████████████████████████████████████████| 30/30 [03:17<00:00,  6.57s/trial, best loss: 0.05012004797457668]\n",
      "100%|███████████████████████████████████████████████| 30/30 [00:22<00:00,  1.32trial/s, best loss: 0.05164263678849886]\n",
      "100%|███████████████████████████████████████████████| 30/30 [05:04<00:00, 10.14s/trial, best loss: 0.04847037441871828]\n",
      "100%|███████████████████████████████████████████████| 30/30 [04:23<00:00,  8.79s/trial, best loss: 0.04992367204234096]\n",
      "100%|███████████████████████████████████████████████| 30/30 [00:21<00:00,  1.37trial/s, best loss: 0.05037244953082368]\n",
      "100%|██████████████████████████████████████████████| 30/30 [04:31<00:00,  9.05s/trial, best loss: 0.047718643107489925]\n",
      "100%|███████████████████████████████████████████████| 30/30 [04:51<00:00,  9.72s/trial, best loss: 0.04955523055606753]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mosin\\anaconda3\\envs\\proteins\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "container = ResultsContainer()\n",
    "for exp_name, exp_params in experiment_params.items():\n",
    "    xs, y, groups, _ = get_data(df, exp_params['features'])\n",
    "    x = xs[exp_params['encoding_type']]\n",
    "    kf = GroupKFold()\n",
    "    print(exp_name)\n",
    "    for train_index, test_index in kf.split(x, y, groups):\n",
    "        # Get Data\n",
    "        x_train, y_train, groups_train = x[train_index], y[train_index], groups[train_index]\n",
    "        x_test, y_test = x[test_index], y[test_index]\n",
    "        cv_params = {\n",
    "            'scoring': 'neg_mean_squared_error',\n",
    "            'cv': GroupKFold(),\n",
    "            'groups': groups_train,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        # Run optimization\n",
    "        params, history = optimize_models(x_train, y_train,\n",
    "                                          optim_params, cv_params,\n",
    "                                          evals=30)\n",
    "        \n",
    "        # Run model comparison\n",
    "        results = compare_models(x_train, y_train, x_test, y_test,\n",
    "                                 models_to_test, optim_params,\n",
    "                                 params)\n",
    "        # Store results\n",
    "        container.update_from_list(exp_name, history, params, results)\n",
    "container.update_file('Results/reproducibility_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-rotation",
   "metadata": {},
   "source": [
    "## Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "boxed-ocean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Counts -----\n",
      "KernelRidge\n",
      "RandomForestRegressor\n",
      "MLPRegressor\n",
      "----- Peptide -----\n",
      "KernelRidge\n",
      "RandomForestRegressor\n",
      "MLPRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mosin\\anaconda3\\envs\\proteins\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Counts+Peptide -----\n",
      "KernelRidge\n",
      "RandomForestRegressor\n",
      "MLPRegressor\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.inspection import permutation_importance\n",
    "\n",
    "# container = pickle.load(open('Results/reproducibility_results.pkl', 'rb'))\n",
    "# results = pickle.load(open('Results/reproducibility_feature_importances.pkl', 'rb'))\n",
    "# for exp_name, exp_params in experiment_params.items():\n",
    "#     print('-'*5, exp_name, '-'*5)\n",
    "#     xs, y, groups, feature_names = get_data(df, exp_params['features'])\n",
    "#     x = xs[exp_params['encoding_type']]\n",
    "#     feature_names = feature_names[-x.shape[1]:]\n",
    "#     kf = GroupKFold()\n",
    "    \n",
    "#     results[exp_name] = {}\n",
    "#     model_results = {}\n",
    "#     for Model in models_to_test:\n",
    "#         model_name = Model.__name__\n",
    "#         print(model_name)\n",
    "#         all_params = container[exp_name][model_name]['params']\n",
    "#         params = []\n",
    "#         for i in range(5):\n",
    "#             p = {}\n",
    "#             for k, v in all_params.items():\n",
    "#                 p[k] = v[i]\n",
    "#             params.append(p)\n",
    "\n",
    "#         model_results = []\n",
    "#         for i, (train_index, test_index) in enumerate(kf.split(x, y, groups)):\n",
    "#             x_train, y_train, groups_train = x[train_index], y[train_index], groups[train_index]\n",
    "#             x_test, y_test = x[test_index], y[test_index]\n",
    "\n",
    "#             model = get_model(Model, optim_params, {model_name: params[i]})[1]\n",
    "#             model.fit(x_train, y_train)\n",
    "#             r = permutation_importance(model, x_test, y_test,\n",
    "#                                        n_repeats=10,\n",
    "#                                        scoring='explained_variance')['importances']\n",
    "#             model_results.append(r)\n",
    "#         model_results = np.hstack(model_results)\n",
    "#         model_results = {f_name:list(vals) for f_name, vals in zip(feature_names, model_results)}\n",
    "#         results[exp_name][model_name] = model_results\n",
    "# pickle.dump(results, open('Results/reproducibility_feature_importances.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-prairie",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interpreted-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_params(optim_history, n):\n",
    "    idxs = np.argsort(optim_history['loss'])[:n]\n",
    "    curr_params = []\n",
    "    params = [{} for i in range(n)]\n",
    "    for i, idx in enumerate(idxs):\n",
    "        for k, v in optim_history.items():\n",
    "            if k != 'loss':\n",
    "                params[i][k] = v[idx]\n",
    "    return params\n",
    "\n",
    "def get_ensemble(models_to_test, models_params, optim_params):\n",
    "    models = []\n",
    "    for Model in models_to_test:\n",
    "        model_name = Model.__name__\n",
    "        for i, p in enumerate(models_params[model_name]):\n",
    "            models.append((model_name+str(i+1), get_model(Model, optim_params, {model_name: p})[1]))\n",
    "    return StackingRegressor(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "based-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# container = pickle.load(open('Results/reproducibility_results.pkl', 'rb'))\n",
    "# # results = pickle.load(open('Results/reproducibility_ensemble.pkl', 'rb'))\n",
    "\n",
    "# results = {}\n",
    "# for exp_name, exp_params in experiment_params.items():\n",
    "#     print('-'*5, exp_name, '-'*5)\n",
    "#     xs, y, groups, feature_names = get_data(df, exp_params['features'])\n",
    "#     x = xs[exp_params['encoding_type']]\n",
    "#     feature_names = feature_names[-x.shape[1]:]\n",
    "#     kf = GroupKFold()\n",
    "    \n",
    "#     exp_results = defaultdict(list)\n",
    "#     for fold, (train_index, test_index) in enumerate(kf.split(x, y, groups)):\n",
    "#         print('Fold:', fold+1)\n",
    "#         x_train, y_train, groups_train = x[train_index], y[train_index], groups[train_index]\n",
    "#         x_test, y_test = x[test_index], y[test_index]\n",
    "        \n",
    "#         models_params = {}\n",
    "#         for Model in models_to_test:\n",
    "#             model_name = Model.__name__\n",
    "#             optim_history = container[exp_name][model_name]['optim_history'][fold]\n",
    "#             models_params[model_name] = get_top_params(optim_history, 2)\n",
    "        \n",
    "#         model = get_ensemble(models_to_test, models_params, optim_params)\n",
    "#         preds_train = model.fit(x_train, y_train).predict(x_train)\n",
    "#         preds_test = model.predict(x_test)\n",
    "        \n",
    "#         metrics = dict(\n",
    "#             MSE=mean_squared_error,\n",
    "#             MAE=mean_absolute_error,\n",
    "#             R2=r2_score,\n",
    "#             EV=explained_variance_score\n",
    "#         )\n",
    "#         for metric, func in metrics.items():\n",
    "#             exp_results['Train '+metric].append(func(y_train, preds_train))\n",
    "#             exp_results['Test '+metric].append(func(y_test, preds_test))\n",
    "#     results[exp_name] = dict(exp_results)\n",
    "\n",
    "# pickle.dump(results, open('Results/reproducibility_ensemble.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
