{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stunning-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from deep import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "completed-crime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peptide</th>\n",
       "      <th>peptide_aromaticity</th>\n",
       "      <th>peptide_gravy</th>\n",
       "      <th>peptide_helix</th>\n",
       "      <th>peptide_instability_index</th>\n",
       "      <th>peptide_isoelectric_point</th>\n",
       "      <th>peptide_len</th>\n",
       "      <th>peptide_log_len</th>\n",
       "      <th>peptide_log_weight</th>\n",
       "      <th>peptide_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>protein_len</th>\n",
       "      <th>protein_log_len</th>\n",
       "      <th>protein_log_weight</th>\n",
       "      <th>protein_molar_extinction_coefficient</th>\n",
       "      <th>protein_sheet</th>\n",
       "      <th>protein_turn</th>\n",
       "      <th>protein_weight</th>\n",
       "      <th>reproducibility</th>\n",
       "      <th>reproducibility_corrected</th>\n",
       "      <th>peptide_detectability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELTQQLNALFQDK</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>-0.742857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>39.107143</td>\n",
       "      <td>4.368781</td>\n",
       "      <td>14</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>7.399267</td>\n",
       "      <td>21.571060</td>\n",
       "      <td>...</td>\n",
       "      <td>480</td>\n",
       "      <td>6.173786</td>\n",
       "      <td>10.925809</td>\n",
       "      <td>14900.0</td>\n",
       "      <td>0.372917</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>55592.8205</td>\n",
       "      <td>0.134116</td>\n",
       "      <td>-0.416929</td>\n",
       "      <td>0.863536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EETGQVLER</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.288889</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>4.252773</td>\n",
       "      <td>9</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>6.966134</td>\n",
       "      <td>23.123484</td>\n",
       "      <td>...</td>\n",
       "      <td>613</td>\n",
       "      <td>6.418365</td>\n",
       "      <td>11.114425</td>\n",
       "      <td>114625.0</td>\n",
       "      <td>0.225122</td>\n",
       "      <td>0.295269</td>\n",
       "      <td>67132.6103</td>\n",
       "      <td>0.118270</td>\n",
       "      <td>-0.276561</td>\n",
       "      <td>0.539523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QLYGDTGVLGR</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.263636</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-20.009091</td>\n",
       "      <td>5.835682</td>\n",
       "      <td>11</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>7.071824</td>\n",
       "      <td>22.152872</td>\n",
       "      <td>...</td>\n",
       "      <td>184</td>\n",
       "      <td>5.214936</td>\n",
       "      <td>9.929104</td>\n",
       "      <td>22982.5</td>\n",
       "      <td>0.288043</td>\n",
       "      <td>0.163043</td>\n",
       "      <td>20518.9427</td>\n",
       "      <td>0.730757</td>\n",
       "      <td>-0.019208</td>\n",
       "      <td>0.858968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EVQVFEITENSAK</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>-0.415385</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>33.061538</td>\n",
       "      <td>4.252773</td>\n",
       "      <td>13</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>7.308954</td>\n",
       "      <td>18.674903</td>\n",
       "      <td>...</td>\n",
       "      <td>188</td>\n",
       "      <td>5.236442</td>\n",
       "      <td>9.948520</td>\n",
       "      <td>4470.0</td>\n",
       "      <td>0.218085</td>\n",
       "      <td>0.202128</td>\n",
       "      <td>20921.2417</td>\n",
       "      <td>0.245870</td>\n",
       "      <td>0.106907</td>\n",
       "      <td>0.876252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EDYICYAR</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.662500</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>62.425000</td>\n",
       "      <td>4.370430</td>\n",
       "      <td>8</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>6.939377</td>\n",
       "      <td>23.851474</td>\n",
       "      <td>...</td>\n",
       "      <td>127</td>\n",
       "      <td>4.844187</td>\n",
       "      <td>9.523820</td>\n",
       "      <td>18450.0</td>\n",
       "      <td>0.188976</td>\n",
       "      <td>0.377953</td>\n",
       "      <td>13681.7806</td>\n",
       "      <td>0.768097</td>\n",
       "      <td>0.377618</td>\n",
       "      <td>0.649913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          peptide  peptide_aromaticity  peptide_gravy  peptide_helix  \\\n",
       "0  SELTQQLNALFQDK             0.071429      -0.742857       0.285714   \n",
       "1       EETGQVLER             0.000000      -1.288889       0.222222   \n",
       "2     QLYGDTGVLGR             0.090909      -0.263636       0.363636   \n",
       "3   EVQVFEITENSAK             0.076923      -0.415385       0.307692   \n",
       "4        EDYICYAR             0.250000      -0.662500       0.375000   \n",
       "\n",
       "   peptide_instability_index  peptide_isoelectric_point  peptide_len  \\\n",
       "0                  39.107143                   4.368781           14   \n",
       "1                  27.300000                   4.252773            9   \n",
       "2                 -20.009091                   5.835682           11   \n",
       "3                  33.061538                   4.252773           13   \n",
       "4                  62.425000                   4.370430            8   \n",
       "\n",
       "   peptide_log_len  peptide_log_weight  peptide_mean  ...  protein_len  \\\n",
       "0         2.639057            7.399267     21.571060  ...          480   \n",
       "1         2.197225            6.966134     23.123484  ...          613   \n",
       "2         2.397895            7.071824     22.152872  ...          184   \n",
       "3         2.564949            7.308954     18.674903  ...          188   \n",
       "4         2.079442            6.939377     23.851474  ...          127   \n",
       "\n",
       "   protein_log_len  protein_log_weight  protein_molar_extinction_coefficient  \\\n",
       "0         6.173786           10.925809                               14900.0   \n",
       "1         6.418365           11.114425                              114625.0   \n",
       "2         5.214936            9.929104                               22982.5   \n",
       "3         5.236442            9.948520                                4470.0   \n",
       "4         4.844187            9.523820                               18450.0   \n",
       "\n",
       "   protein_sheet  protein_turn  protein_weight  reproducibility  \\\n",
       "0       0.372917      0.162500      55592.8205         0.134116   \n",
       "1       0.225122      0.295269      67132.6103         0.118270   \n",
       "2       0.288043      0.163043      20518.9427         0.730757   \n",
       "3       0.218085      0.202128      20921.2417         0.245870   \n",
       "4       0.188976      0.377953      13681.7806         0.768097   \n",
       "\n",
       "  reproducibility_corrected  peptide_detectability  \n",
       "0                 -0.416929               0.863536  \n",
       "1                 -0.276561               0.539523  \n",
       "2                 -0.019208               0.858968  \n",
       "3                  0.106907               0.876252  \n",
       "4                  0.377618               0.649913  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/reproducibility.csv').sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "departmental-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsContainer:\n",
    "    def __init__(self, path=None):\n",
    "        self.container = defaultdict(\n",
    "            lambda: defaultdict(\n",
    "                lambda: dict(\n",
    "                    optim_history=[],\n",
    "                    params=defaultdict(list),\n",
    "                    results=defaultdict(list),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        if path is not None:\n",
    "            self.load(path)\n",
    "            \n",
    "    def update_from_list(self, exp_name, optim_history, params, results):\n",
    "        for model_name in optim_history:\n",
    "            self.update(exp_name, model_name,\n",
    "                        optim_history[model_name],\n",
    "                        params[model_name],\n",
    "                        results[model_name])\n",
    "            \n",
    "    def update(self, exp_name, model_name, optim_history, params, results):\n",
    "        self.container[exp_name][model_name]['optim_history'].append(optim_history)\n",
    "        for k, v in params.items():\n",
    "            self.container[exp_name][model_name]['params'][k].append(v)\n",
    "        for k, v in results.items():\n",
    "            self.container[exp_name][model_name]['results'][k].append(v)\n",
    "        \n",
    "    def save(self, path):\n",
    "        pickle.dump(self.to_dict(), open(path, 'wb'))\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.container.update(pickle.load(open(path, 'rb')))\n",
    "        \n",
    "    def update_file(self, path):\n",
    "        container = ResultsContainer(path)\n",
    "        container_new = self.container\n",
    "        for exp_name, models in container_new.items():\n",
    "            for model_name, model_container in models.items():\n",
    "                container[exp_name][model_name] = model_container\n",
    "        container.save(path)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.rec_to_dict(self.container)\n",
    "    \n",
    "    def rec_to_dict(self, d):\n",
    "        if type(d)!=defaultdict and type(d)!=dict:\n",
    "            return d\n",
    "        for k, v in d.items():\n",
    "            if type(v)==defaultdict or type(v)==dict:\n",
    "                d[k] = self.rec_to_dict(v)\n",
    "        return dict(d)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.container[key]\n",
    "    \n",
    "    \n",
    "class Objective:\n",
    "    def __init__(self, x, y, groups, model_getter, Dataset, collate, epochs=100, early_stop=10):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.groups = groups\n",
    "        self.Dataset = Dataset\n",
    "        self.collate = collate\n",
    "        self.get_model = model_getter\n",
    "        self.epochs = epochs\n",
    "        self.early_stop = early_stop\n",
    "\n",
    "    def __call__(self, space):\n",
    "        params = space.copy()\n",
    "        batch_size = params['batch_size']\n",
    "        del params['batch_size']\n",
    "        model = self.get_model(**params)\n",
    "        \n",
    "        loader_params = {\n",
    "            'Dataset': self.Dataset,\n",
    "            'collate': self.collate,\n",
    "            'batch_size': batch_size,\n",
    "        }\n",
    "        scores = parallel_net_cross_val(model, self.x, self.y, loader_params,\n",
    "                                        groups=self.groups, kf=GroupKFold(),\n",
    "                                        epochs=self.epochs, early_stop=self.early_stop)\n",
    "        return scores.mean()\n",
    "\n",
    "\n",
    "def optimize_models(x, y, groups, optim_params, exp_params, evals=1, epochs=1, early_stop=10):\n",
    "    optim_results, history_results = {}, {}\n",
    "    for model_name, model_params in optim_params.items():\n",
    "        params, history = optimize(\n",
    "            objective=Objective(x, y, groups,\n",
    "                                model_getter=model_params['model_getter'],\n",
    "                                **exp_params, epochs=epochs, early_stop=early_stop),\n",
    "            space=model_params['space'],\n",
    "            mapping=model_params['mapping'],\n",
    "            evals=evals\n",
    "        )\n",
    "        optim_results[model_name] = params\n",
    "        history_results[model_name] = history\n",
    "    return optim_results, history_results\n",
    "\n",
    "def evaluate(x_train, y_train, x_test, y_test, model_getter, params, exp_params, epochs=1, early_stop=10):\n",
    "    params = params.copy()\n",
    "    batch_size = params['batch_size']\n",
    "    del params['batch_size']\n",
    "    model = model_getter(**params)\n",
    "    train_loader, test_loader = get_loaders(x_train, y_train,\n",
    "                                            x_test, y_test,\n",
    "                                            batch_size=batch_size,\n",
    "                                            **exp_params)\n",
    "    model.train(train_loader, epochs=epochs, early_stop=early_stop, verbose=False)\n",
    "    # Get preds and targets\n",
    "    preds_train, targets_train = model.predict_from_loader(train_loader, to_numpy=True)\n",
    "    preds_test, targets_test = model.predict_from_loader(test_loader, to_numpy=True)\n",
    "    # Get metrics\n",
    "    metrics = dict(\n",
    "        MSE=mean_squared_error,\n",
    "        MAE=mean_absolute_error,\n",
    "        R2=r2_score,\n",
    "        EV=explained_variance_score\n",
    "    )\n",
    "    results = {}\n",
    "    for metric, func in metrics.items():\n",
    "        results['Train '+metric] = func(targets_train, preds_train)\n",
    "        results['Test '+metric] = func(targets_test, preds_test)\n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_models(x_train, y_train, x_test, y_test,\n",
    "                   optim_params, optim_results, epx_params,\n",
    "                   epochs=1, early_stop=10):\n",
    "    results = {}\n",
    "    for model_name, model_params in optim_params.items():\n",
    "        model_getter = optim_params[model_name]['model_getter']\n",
    "        results[model_name] = evaluate(\n",
    "            x_train, y_train, x_test, y_test,\n",
    "            model_getter, optim_results[model_name],\n",
    "            exp_params, epochs, early_stop\n",
    "        )\n",
    "    return results\n",
    "\n",
    "def run_experiment(x, y, groups, exp_name, exp_params, optim_params,\n",
    "                   evals, epochs, early_stop=10):\n",
    "    container = ResultsContainer()\n",
    "    kf = GroupKFold()\n",
    "    for train_index, test_index in kf.split(x, y, groups):\n",
    "        # Get train/test data\n",
    "        x_train, y_train, groups_train = x[train_index], y[train_index], groups[train_index]\n",
    "        x_test, y_test = x[test_index], y[test_index]\n",
    "        # Run optimization\n",
    "        params, history = optimize_models(x_train, y_train, groups_train,\n",
    "                                          optim_params, exp_params, evals=evals,\n",
    "                                          epochs=epochs, early_stop=early_stop)\n",
    "        # Run model comparison\n",
    "        results = compare_models(x_train, y_train, x_test, y_test,\n",
    "                                 optim_params, params, exp_params,\n",
    "                                 epochs=epochs, early_stop=early_stop)\n",
    "        # Store results\n",
    "        container.update_from_list(exp_name, history, params, results)\n",
    "    return container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-suite",
   "metadata": {},
   "source": [
    "# Seq experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "distinct-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'Seq'\n",
    "exp_params = dict(\n",
    "    Dataset=PeptideDataset,\n",
    "    collate=Collate(),\n",
    ")\n",
    "optim_params = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-airfare",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "solid-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_getter(output_dim, num_layers, dropout, lr, weight_decay, optimizer):\n",
    "    net = nn.Sequential(\n",
    "        RecurrentEncoder(\n",
    "            input_dim=len(aminoacids)+2,\n",
    "            embedding_dim=output_dim//2,\n",
    "            output_dim=output_dim,\n",
    "            bidirectional=True,\n",
    "            rnn_type='gru',\n",
    "            pool_type='avg',\n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout,\n",
    "        ),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(output_dim, 1),\n",
    "        nn.Sigmoid()\n",
    "    ).to(device)\n",
    "    \n",
    "    if optimizer == 'sgd':\n",
    "        optim = torch.optim.SGD(net.parameters(), lr=lr,\n",
    "                                momentum=0.9, nesterov=True, \n",
    "                                weight_decay=weight_decay)\n",
    "    else:\n",
    "        optim = torch.optim.AdamW(net.parameters(), lr=lr,\n",
    "                                  weight_decay=weight_decay)\n",
    "    model = Network(\n",
    "        net=net,\n",
    "        optimizer=optim,\n",
    "        loss=nn.BCELoss(),\n",
    "        gamma=0.97\n",
    "    )\n",
    "    return model\n",
    "\n",
    "gru_space = {\n",
    "    'output_dim': hyperopt.hp.qloguniform('output_dim', np.log(4), np.log(128), 4),\n",
    "    'num_layers': hyperopt.hp.quniform('num_layers', 1, 3, 1),\n",
    "    'dropout': hyperopt.hp.uniform('dropout', 0, 0.5),\n",
    "    'batch_size': hyperopt.hp.qloguniform('batch_size', np.log(8), np.log(128), 4),\n",
    "    'lr': hyperopt.hp.loguniform('lr', np.log(1e-5), np.log(0.1)),\n",
    "    'weight_decay': hyperopt.hp.loguniform('weight_decay', np.log(1e-5), np.log(1)),\n",
    "    'optimizer': hyperopt.hp.choice('optimizer', [0, 1])\n",
    "}\n",
    "gru_mapping = dict(\n",
    "    output_dim=lambda x: int(x),\n",
    "    num_layers=lambda x: int(x),\n",
    "    batch_size=lambda x: int(x),\n",
    "    optimizer=lambda x: ['sgd', 'adam'][x]\n",
    ")\n",
    "\n",
    "optim_params['GRU'] = dict(\n",
    "    model_getter=gru_getter,\n",
    "    space=gru_space,\n",
    "    mapping=gru_mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-latin",
   "metadata": {},
   "source": [
    "### CNNGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coupled-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnngru_getter(output_dim, num_layers, dropout, lr, weight_decay, optimizer):\n",
    "    net = nn.Sequential(\n",
    "        ConvolutionalRecurrentEncoder(\n",
    "            input_dim=len(aminoacids)+2,\n",
    "            output_dim=output_dim,\n",
    "            bidirectional=True,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "        ),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(output_dim, 1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "    \n",
    "    if optimizer == 'sgd':\n",
    "        optim = torch.optim.SGD(net.parameters(), lr=lr,\n",
    "                                momentum=0.9, nesterov=True, \n",
    "                                weight_decay=weight_decay)\n",
    "    else:\n",
    "        optim = torch.optim.AdamW(net.parameters(), lr=lr,\n",
    "                                  weight_decay=weight_decay)\n",
    "    model = Network(\n",
    "        net=net,\n",
    "        optimizer=optim,\n",
    "        loss=nn.BCELoss(),\n",
    "        gamma=0.97\n",
    "    )\n",
    "    return model\n",
    "        \n",
    "cnngru_space = {\n",
    "    'output_dim': hyperopt.hp.qloguniform('output_dim', np.log(4), np.log(128), 4),\n",
    "    'num_layers': hyperopt.hp.quniform('num_layers', 1, 3, 1),\n",
    "    'dropout': hyperopt.hp.uniform('dropout', 0, 0.5),\n",
    "    'batch_size': hyperopt.hp.qloguniform('batch_size', np.log(8), np.log(256), 4),\n",
    "    'lr': hyperopt.hp.loguniform('lr', np.log(1e-4), np.log(0.1)),\n",
    "    'weight_decay': hyperopt.hp.loguniform('weight_decay', np.log(1e-5), np.log(1)),\n",
    "    'optimizer': hyperopt.hp.choice('optimizer', [0, 1])\n",
    "}\n",
    "cnngru_mapping = dict(\n",
    "    output_dim=lambda x: int(x),\n",
    "    num_layers=lambda x: int(x),\n",
    "    batch_size=lambda x: int(x),\n",
    "    optimizer=lambda x: ['sgd', 'adam'][x]\n",
    ")\n",
    "\n",
    "optim_params['CNNGRU'] = dict(\n",
    "    model_getter=cnngru_getter,\n",
    "    space=cnngru_space,\n",
    "    mapping=cnngru_mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-tennessee",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expanded-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_getter(output_dim, num_layers, dropout, lr, weight_decay, optimizer):\n",
    "    net = nn.Sequential(\n",
    "        TransformerEncoder(\n",
    "            input_dim=len(aminoacids)+2,\n",
    "            output_dim = output_dim,\n",
    "            hidden_dim = output_dim*2,\n",
    "            kind='bert',\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        ),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(output_dim, 1),\n",
    "        nn.Sigmoid()\n",
    "    ).to(device)\n",
    "    \n",
    "    if optimizer == 'sgd':\n",
    "        optim = torch.optim.SGD(net.parameters(), lr=lr,\n",
    "                                momentum=0.9, nesterov=True, \n",
    "                                weight_decay=weight_decay)\n",
    "    else:\n",
    "        optim = torch.optim.AdamW(net.parameters(), lr=lr,\n",
    "                                  weight_decay=weight_decay)\n",
    "    model = Network(\n",
    "        net=net,\n",
    "        optimizer=optim,\n",
    "        loss=nn.BCELoss(),\n",
    "        gamma=0.97\n",
    "    )\n",
    "    return model\n",
    "        \n",
    "bert_space = {\n",
    "    'output_dim': hyperopt.hp.qloguniform('output_dim', np.log(4), np.log(128), 4),\n",
    "    'num_layers': hyperopt.hp.quniform('num_layers', 1, 3, 1),\n",
    "    'num_heads': hyperopt.hp.quniform('num_layers', 1, 4, 2),\n",
    "    'dropout': hyperopt.hp.uniform('dropout', 0, 0.5),\n",
    "    'batch_size': hyperopt.hp.qloguniform('batch_size', np.log(8), np.log(256), 4),\n",
    "    'lr': hyperopt.hp.loguniform('lr', np.log(1e-4), np.log(0.1)),\n",
    "    'weight_decay': hyperopt.hp.loguniform('weight_decay', np.log(1e-5), np.log(1)),\n",
    "    'optimizer': hyperopt.hp.choice('optimizer', [0, 1])\n",
    "}\n",
    "bert_mapping = dict(\n",
    "    output_dim=lambda x: int(x),\n",
    "    num_layers=lambda x: int(x),\n",
    "    num_heads=lambda x: int(x),\n",
    "    batch_size=lambda x: int(x),\n",
    "    optimizer=lambda x: ['sgd', 'adam'][x]\n",
    ")\n",
    "\n",
    "optim_params['BERT'] = dict(\n",
    "    model_getter=bert_getter,\n",
    "    space=bert_space,\n",
    "    mapping=bert_mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-nightlife",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "southern-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'GRU',\n",
    "#     'CNNGRU',\n",
    "#     'BERT',\n",
    "]\n",
    "\n",
    "optim_params = {k:optim_params[k] for k in models}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
