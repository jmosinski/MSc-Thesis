{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assigned-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from deep import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "finished-district",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peptide</th>\n",
       "      <th>detectability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLSEVEELNMSLTALREK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ERMDEEQKLYTD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YVPRAVLVDLEPGTMDSIR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TAHYGSLPQKSHGR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KFVADGIFK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               peptide  detectability\n",
       "0   LLSEVEELNMSLTALREK              0\n",
       "1         ERMDEEQKLYTD              0\n",
       "2  YVPRAVLVDLEPGTMDSIR              0\n",
       "3       TAHYGSLPQKSHGR              1\n",
       "4            KFVADGIFK              1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'batch_size': 132,\n",
    "    'dropout': 0.449853682405601,\n",
    "    'lr': 0.00048433947922833076,\n",
    "    'num_layers': 3,\n",
    "    'output_dim': 140,\n",
    "    'weight_decay': 2.9229430030464116e-05\n",
    "}\n",
    "\n",
    "df = pd.read_csv('../Data/detectability_homo.csv')\n",
    "df2 = pd.read_csv('../Data/detectability_mus.csv')\n",
    "x2, y2 = df2[['peptide', 'detectability']].values.T\n",
    "\n",
    "df_train = df.iloc[:67000]\n",
    "df_val = df.iloc[67000:70000]\n",
    "df_test = df.iloc[70000:]\n",
    "x_train, y_train = df_train[['peptide', 'detectability']].values.T\n",
    "x_val, y_val = df_val[['peptide', 'detectability']].values.T\n",
    "x_test, y_test = df_test[['peptide', 'detectability']].values.T\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "joint-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transfer_learning(encoder, loader):\n",
    "    embeds, targets = get_embeddings(encoder, loader)\n",
    "    scores = cross_val_score(KNeighborsClassifier(30),\n",
    "                             embeds, targets,\n",
    "                             cv=10, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "def run_encoder_experiments(encoder, name):\n",
    "    # Encoder Metrics\n",
    "    metrics = test_encoder(encoder,\n",
    "                           x_train, y_train,\n",
    "                           x_test, y_test)\n",
    "\n",
    "    # Encoder metrics transfer learning\n",
    "    loader2 = DataLoader(\n",
    "        PeptideDataset(x2, y2, aminoacids),\n",
    "        batch_size=1024,\n",
    "        shuffle=False,\n",
    "        collate_fn=Collate()\n",
    "    )\n",
    "    metrics['Transfer ACC'] = test_transfer_learning(encoder, loader2)\n",
    "    pickle.dump(metrics, open(f'detectability_{name}_encoder_metrics.pkl', 'wb'))\n",
    "    print('Encoder metrics: ')\n",
    "    for k, v in metrics.items():\n",
    "        print(k, v)\n",
    "\n",
    "    # Embeddings visualization\n",
    "    plt.figure()\n",
    "    loader = DataLoader(\n",
    "        PeptideDataset(x_test, y_test, aminoacids),\n",
    "        batch_size=1024,\n",
    "        shuffle=False,\n",
    "        collate_fn=Collate()\n",
    "    )\n",
    "    embeds, targets = get_embeddings(encoder, loader)\n",
    "    visualize_embeddings(embeds, targets, palette=None)\n",
    "    plt.savefig(f'detectability_{name}_embeddings.jpg', dpi=300)\n",
    "\n",
    "    # Embeddings visualization transfer learning\n",
    "    plt.figure()\n",
    "    embeds, targets = get_embeddings(encoder, loader2)\n",
    "    visualize_embeddings(embeds, targets)\n",
    "    plt.savefig(f'detectability_{name}_embeddings2.jpg', dpi=300)\n",
    "    \n",
    "def test_sample_efficiency(eval_fn, x_train, y_train, x_test, y_test, levels):\n",
    "    results = defaultdict(list)\n",
    "    for level in levels:\n",
    "        n_t = int(0.9*level)\n",
    "        n_v = (level - n_t)\n",
    "        x_t, y_t = x_train[:n_t], y_train[:n_t]\n",
    "        x_v, y_v = x_train[n_t:n_t+n_v], y_train[n_t:n_t+n_v]\n",
    "        results['n_samples'].append(level)\n",
    "        eval_results = eval_fn(x_t, y_t, x_v, y_v, x_test, y_test)\n",
    "        for k, v in eval_results.items():\n",
    "            results[k].append(v)\n",
    "    return dict(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-thanksgiving",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "remarkable-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spervised(x_train, y_train, x_test, y_test,\n",
    "                    output_dim, num_layers, dropout,\n",
    "                    batch_size, lr, weight_decay,\n",
    "                    epochs=1, early_stop=float('inf')):\n",
    "    \n",
    "    train_dataset = PeptideDataset(x_train, y_train, aminoacids)\n",
    "    test_dataset = PeptideDataset(x_test, y_test, aminoacids)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=Collate(),\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=1024,\n",
    "        shuffle=False,\n",
    "        collate_fn=Collate(),\n",
    "    )\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        RecurrentEncoder(\n",
    "            input_dim=len(aminoacids)+2,\n",
    "            embedding_dim=output_dim//2,\n",
    "            output_dim=output_dim,\n",
    "            bidirectional=True,\n",
    "            rnn_type='gru',\n",
    "            pool_type='avg',\n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout,\n",
    "        ),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(output_dim, 1),\n",
    "        nn.Sigmoid()\n",
    "    ).to(device)\n",
    "        \n",
    "    model = Network(\n",
    "        net=net,\n",
    "        optimizer=torch.optim.AdamW(net.parameters(), lr=lr,\n",
    "                                    weight_decay=weight_decay),\n",
    "        loss=nn.BCELoss(),\n",
    "        gamma=0.97,\n",
    "        path='supervised_gru.pth'\n",
    "    )\n",
    "\n",
    "    history = model.train(train_loader, \n",
    "                          test_loader=test_loader,\n",
    "                          epochs=epochs,\n",
    "                          early_stop=early_stop,\n",
    "                          verbose=False)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "resident-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training\n",
    "# model, history = train_spervised(x_train, y_train, x_val, y_val,\n",
    "#                                  **params, epochs=1, early_stop=10)\n",
    "# history.to_csv('detectability_supervised_training.csv')\n",
    "# plt.figure()\n",
    "# history.plot()\n",
    "\n",
    "# # Classifier metrics\n",
    "# metrics = test_classifier(model,\n",
    "#                           x_train, y_train,\n",
    "#                           x_test, y_test)\n",
    "# pickle.dump(metrics, open('detectability_supervised_clf_metrics.pkl', 'wb'))\n",
    "# print('Classification Metrics:')\n",
    "# for k, v in metrics.items():\n",
    "#     print(k, v)\n",
    "# print()\n",
    "\n",
    "# run_encoder_experiments(model.net[0], 'supervised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "racial-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_fn(x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "#     model, history = train_spervised(x_train, y_train, x_val, y_val,\n",
    "#                                      **params, epochs=1, early_stop=10)\n",
    "#     return test_classifier(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# levels = [1000*2**i for i in range(7)]\n",
    "# results = test_sample_efficiency(eval_fn, x_train, y_train, x_test, y_test, levels)\n",
    "# pickle.dump(results, open('detectability_supervised_sample_efficiency.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-canyon",
   "metadata": {},
   "source": [
    "# Triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "assured-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_triplet(x_train, y_train, x_test, y_test,\n",
    "                  output_dim, num_layers, dropout,\n",
    "                  batch_size, lr, weight_decay,\n",
    "                  n_train=1e2, n_test=1e2 ,\n",
    "                  epochs=1, early_stop=float('inf')):\n",
    "    \n",
    "    train_dataset = PeptideTripletDataset(x_train, y_train, n_train, aminoacids)\n",
    "    test_dataset = PeptideTripletDataset(x_test, y_test, n_test, aminoacids)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=CollateTriplet(),\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=1024,\n",
    "        shuffle=False,\n",
    "        collate_fn=CollateTriplet(),\n",
    "    )\n",
    "\n",
    "    net = SiameseNet(\n",
    "        RecurrentEncoder(\n",
    "            input_dim=len(aminoacids)+2,\n",
    "            embedding_dim=output_dim//2,\n",
    "            output_dim=output_dim,\n",
    "            bidirectional=True,\n",
    "            rnn_type='gru',\n",
    "            pool_type='avg',\n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout,\n",
    "        ),\n",
    "    ).to(device)\n",
    "        \n",
    "    model = Network(\n",
    "        net=net,\n",
    "        optimizer=torch.optim.AdamW(net.parameters(), lr=lr,\n",
    "                                    weight_decay=weight_decay),\n",
    "        loss=TripletLoss(),\n",
    "        gamma=0.9,\n",
    "        path='triplet_gru.pth'\n",
    "    )\n",
    "\n",
    "    history = model.train(train_loader, \n",
    "                          test_loader=test_loader,\n",
    "                          epochs=epochs,\n",
    "                          early_stop=early_stop,\n",
    "                          verbose=False)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "excited-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, history = train_triplet(x_train, y_train, x_val, y_val,\n",
    "#                                **params, n_train=3e5, n_test=1e4,\n",
    "#                                epochs=30, early_stop=3)\n",
    "# history.to_csv('detectability_triplet_training.csv')\n",
    "# plt.figure()\n",
    "# history.plot()\n",
    "\n",
    "# run_encoder_experiments(model.net.encoder, 'triplet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "material-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_fn(x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "#     n_t, n_v = len(y_train), len(y_val)\n",
    "#     model, history = train_triplet(x_train, y_train, x_val, y_val,\n",
    "#                                    **params, n_train=3*n_t, n_test=3*n_val,\n",
    "#                                    epochs=30, early_stop=3)\n",
    "#     return test_encoder(model.net.encoder, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# levels = [1000*2**i for i in range(7)]\n",
    "# results = test_sample_efficiency(eval_fn, x_train, y_train, x_test, y_test, levels)\n",
    "# pickle.dump(results, open('detectability_triplet_sample_efficiency.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-manchester",
   "metadata": {},
   "source": [
    "# Supervised Triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "composite-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised_triplet(x_train, y_train, x_test, y_test,\n",
    "                             output_dim, num_layers, dropout,\n",
    "                             batch_size, lr, weight_decay,\n",
    "                             n_train=1e2, n_test=1e2,\n",
    "                             epochs=1, early_stop=float('inf')):\n",
    "    \n",
    "    train_dataset = PeptideTripletDataset(x_train, y_train, n_train, aminoacids)\n",
    "    test_dataset = PeptideTripletDataset(x_test, y_test, n_test, aminoacids)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=CollateSupervisedTriplet(),\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=1024,\n",
    "        shuffle=False,\n",
    "        collate_fn=CollateSupervisedTriplet(),\n",
    "    )\n",
    "\n",
    "    net = SupervisedSiameseNet(\n",
    "        encoder = RecurrentEncoder(\n",
    "            input_dim=len(aminoacids)+2,\n",
    "            embedding_dim=output_dim//2,\n",
    "            output_dim=output_dim,\n",
    "            bidirectional=True,\n",
    "            rnn_type='gru',\n",
    "            pool_type='avg',\n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout,\n",
    "        ),\n",
    "        predictor = nn.Sequential(\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(output_dim, 1),\n",
    "        nn.Sigmoid()\n",
    "        )\n",
    "    ).to(device)\n",
    "        \n",
    "    model = Network(\n",
    "        net=net,\n",
    "        optimizer=torch.optim.AdamW(net.parameters(), lr=lr,\n",
    "                                    weight_decay=weight_decay),\n",
    "        loss=SSLoss(\n",
    "            embed_loss=TripletLoss(),\n",
    "            pred_loss=nn.BCELoss(),\n",
    "        ),\n",
    "        gamma=0.9,\n",
    "        path='supervised_triplet_gru.pth'\n",
    "    )\n",
    "\n",
    "    history = model.train(train_loader, \n",
    "                          test_loader=test_loader,\n",
    "                          epochs=epochs,\n",
    "                          early_stop=early_stop)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "recreational-reverse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, history = train_supervised_triplet(x_train, y_train, x_val, y_val,\n",
    "#                                           **params, n_train=3e5, n_test=1e4,\n",
    "#                                           epochs=30, early_stop=3)\n",
    "# history.to_csv('detectability_supervised_triplet_training.csv')\n",
    "# plt.figure()\n",
    "# history.plot()\n",
    "\n",
    "# run_encoder_experiments(model.net.encoder, 'supervised_triplet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "structured-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_fn(x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "#     n_t, n_v = len(y_train), len(y_val)\n",
    "#     model, history = train_supervised_triplet(x_train, y_train, x_val, y_val,\n",
    "#                                               **params, n_train=3*n_t, n_test=3*n_val,\n",
    "#                                               epochs=30, early_stop=3)\n",
    "#     return test_encoder(model.net.encoder, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# levels = [1000*2**i for i in range(7)]\n",
    "# results = test_sample_efficiency(eval_fn, x_train, y_train, x_test, y_test, levels)\n",
    "# pickle.dump(results, open('detectability_supervised_triplet_sample_efficiency.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-authentication",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reduced-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(x_train, y_train,\n",
    "                   output_dim, num_layers, dropout,\n",
    "                   batch_size, lr, weight_decay,\n",
    "                   epochs=1, early_stop=float('inf')):\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        dataset=PeptideDataset(x_train, y_train, aminoacids),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=Collate(),\n",
    "    )\n",
    "#     test_loader = DataLoader(\n",
    "#         dataset=PeptideDataset(x_test, y_test, aminoacids),\n",
    "#         batch_size=1024,\n",
    "#         shuffle=False,\n",
    "#         collate_fn=Collate(),\n",
    "#     )\n",
    "    \n",
    "    \n",
    "    nets = []\n",
    "    for rnn_type in ['gru', 'lstm']:\n",
    "        for pool_type in ['avg', 'max', 'last']:\n",
    "            nets.append(\n",
    "                nn.Sequential(\n",
    "                    RecurrentEncoder(\n",
    "                        input_dim=len(aminoacids)+2,\n",
    "                        embedding_dim=output_dim//2,\n",
    "                        output_dim=output_dim,\n",
    "                        bidirectional=True,\n",
    "                        rnn_type=rnn_type,\n",
    "                        pool_type=pool_type,\n",
    "                        num_layers=num_layers, \n",
    "                        dropout=dropout,\n",
    "                    ),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(output_dim, 1),\n",
    "                    nn.Sigmoid()\n",
    "                ).to(device)\n",
    "            )\n",
    "    \n",
    "    models = []\n",
    "    for i, net in enumerate(nets):\n",
    "        models.append(\n",
    "            Network(\n",
    "                net=net,\n",
    "                optimizer=torch.optim.AdamW(net.parameters(), lr=lr,\n",
    "                                            weight_decay=weight_decay),\n",
    "                loss=nn.BCELoss(),\n",
    "                gamma=0.97,\n",
    "                path=f'model{i}.pth'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    model = VotingEnsemble(models)\n",
    "    history = model.train(train_loader,\n",
    "                          epochs=epochs, early_stop=early_stop,\n",
    "                          verbose=False)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "biological-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training\n",
    "# model = train_ensemble(np.hstack([x_train, x_val]),\n",
    "#                        np.hstack([y_train, y_val]),\n",
    "#                        **params, epochs=3)\n",
    "\n",
    "# # Classifier metrics\n",
    "# metrics = test_classifier(model,\n",
    "#                           np.hstack([x_train, x_val]),\n",
    "#                           np.hstack([y_train, y_val]),\n",
    "#                           x_test, y_test)\n",
    "\n",
    "# # pickle.dump(metrics, open('detectability_ensemble_clf_metrics.pkl', 'wb'))\n",
    "# print('Classification Metrics:')\n",
    "# for k, v in metrics.items():\n",
    "#     print(k, v)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_fn(x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "#     model, history = train_ensemble(x_train, y_train, x_val, y_val,\n",
    "#                                     **params, epochs=50, early_stop=100)\n",
    "#     return test_classifier(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# levels = [1000*2**i for i in range(7)]\n",
    "# results = test_sample_efficiency(eval_fn, x_train, y_train, x_test, y_test, levels)\n",
    "# pickle.dump(results, open('detectability_ensemble_sample_efficiency.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
